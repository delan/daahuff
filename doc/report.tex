\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{multirow}
\usepackage[usenames,dvipsnames]{color}
\hypersetup{
	colorlinks,
	pdfauthor=Delan Azabani,
	pdftitle=Design and Analysis of Algorithms 300: Huffman assignment
}
\lstset{basicstyle=\ttfamily, basewidth=0.5em}

\title{Design and Analysis of Algorithms 300\\Huffman assignment}
\date{April 19, 2014}
\author{Delan Azabani}

\begin{document}

\maketitle

\section{Implemented functionality}

Frequency table generation is supported, as are both compression and
decompression with the Huffman coding scheme. Symbols of arbitrary length are
supported, only in the sense that Unicode characters that are represented by
multiple octets in UTF-8 are intuitively treated as single symbols.

\section{Known defects}

While there is a check for missing symbols during compression, there is no such
equivalent guard in the decompression routine. If the Huffman tree is
inappropriate for the input data, the program will likely fall into an infinite
loop, attempting to find a leaf node.

Decompression \emph{may} yield a few more superfluous symbols than one might
expect, but still a negligible number regardless. This is because of the way
the compressed data are handled: the \texttt{BitArray} is converted to a
\texttt{byte[]} which pads the data with up to 7 bits, \emph{then} the data are
encoded with Base64. On the other hand, § 1.3 of the specification may imply a
direct conversion to 6-bit symbols --- padding the data with up to 5 bits.

\section{Build information}

This implementation is based on the foundation supplied as
\texttt{DAA300Asgn\_Base.zip} with minor changes. While it was developed using
Visual C\# 2013, it should also compile and run on Visual C\# 2010 without
issues. To run this assignment, open \texttt{Asgn.sln} and hit F5 within Visual
Studio.

\newpage

\section{Design considerations regarding data types}

One of my goals when completing this assignment was to ensure that the core
Huffman and Base64 algorithms only ever interact with sequences of octets
(\texttt{byte[]}) --- beyond making them agnostic of text encodings, this would
allow them to be used with other data which may not necessarily be textual.

Of course, the implementation still needed to conform to the specification, and
behave as expected when tested with copied textual input. This led to the need
to make careful decisions as to where and how the interactions between strings
and octets are made.

The .NET Framework defines \texttt{System.Char} as a `UTF-16 code unit', which
is obviously not an octet. Since the release of Unicode 2.0 in 1996 however,
these 16-bit values no longer necessarily represent entire Unicode code points
either. For instance:

\begin{center}
\begin{figure}[h]
	\begin{center}
	\begin{tabular}{|lllll|}
		\hline
		Glyph &
			Unicode code point &
			UTF-32 &
			UTF-16 \texttt{char} &
			UTF-8 \\\hline
		\raisebox{-3pt}{
			\includegraphics[height=12pt]{U+000041.png}
		} &
			\texttt{U+000041} &
			\texttt{00000041} &
			\texttt{0041} &
			\texttt{41} \\\hline
		\raisebox{-3pt}{
			\includegraphics[height=12pt]{U+0000A9.png}
		} &
			\texttt{U+0000A9} &
			\texttt{000000A9} &
			\texttt{00A9} &
			\texttt{C2 A9} \\\hline
		\raisebox{-3pt}{
			\includegraphics[height=12pt]{U+002202.png}
		} &
			\texttt{U+002202} &
			\texttt{00002202} &
			\texttt{2202} &
			\texttt{E2 88 82} \\\hline
		\raisebox{-3pt}{
			\includegraphics[height=12pt]{U+01F530.png}
		} &
			\texttt{U+01F530} &
			\texttt{0001F530} &
			\texttt{D83D DD30} &
			\texttt{F0 9F 94 B0} \\\hline
	\end{tabular}
	\end{center}
	\caption{
		A selection of Unicode characters and ways to encode
		them as octets.
	}
	\label{UC}
\end{figure}
\end{center}

Furthermore, the code units in the UTF-16 and UTF-32 encodings need to have a
defined \emph{endianness} when serialised into a sequence of octets. To avoid
the need to consider this, I chose UTF-8 as the internal representation of any
text where possible, only using \texttt{string} and \texttt{char} when dealing
with the UI. Among other perks, UTF-8 also provides the unique benefit of
being totally backwards compatible with ASCII.

To facilitate this, I had to partially implement § 2.2 of the assignment, to
the extent that characters represented by multiple octets in UTF-8 shall be
treated as single symbols in frequency tables. For instance, the sequence
\texttt{F0 9F 94 B0} would be treated as a single symbol. This only considers
frequency tables read from the UI.

How would I \emph{generate} a useful frequency table for text, if all I am
dealing with are sequences of octets? The approach used for binary data would
yield four separate entries for the character \includegraphics[height=12pt]
{U+01F530.png} --- \texttt{F0}, \texttt{9F}, \texttt{94} and \texttt{B0}. While
this is how a compression tool would work in real life, it is more confusing in
the context of this assignment.

My solution was to write two stubs each, for the generators and compressors:

\begin{itemize}
	\item
		one which deals purely in octets, for binary data
	\item
		one which is aware that the octets are a UTF-8 sequence
\end{itemize}

This is still better than a naïve approach where all of the algorithms use
\texttt{char} values. Combining such a configuration with a character like
\includegraphics[height=12pt]{U+01F530.png} would lead to disastrous results.
While \texttt{D83D} and \texttt{DD30} are not valid characters, they would
receive separate symbols in the frequency table, causing the table to fail to
correctly display in the UI.

The result of all of this over-engineering is an implementation that is
flexible enough to be used with binary data, while supporting the nuances of
Unicode well enough to yield minimal surprising behaviour when the GUI is used.

\end{document}
